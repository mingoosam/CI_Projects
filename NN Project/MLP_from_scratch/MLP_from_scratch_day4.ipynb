{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459e340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d01cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why batches?\n",
    "\n",
    "# allows us to compute in parallel\n",
    "# primarily though, it helps with generalization: think of inputs = [1.0, 2.0, 3.0, 2.5] \n",
    "# inputs is a single sample with four features. if we pass a batch of samples instead of just one,\n",
    "# we give it the opportunity to generalize.\n",
    "\n",
    "# at 5:20, we see the fitment line bouncing around as it's exposed to just one sample at a time\n",
    "# less bouncing as we increase batch size\n",
    "\n",
    "# if we show all the samples at once, we get overfitting. does well with training data, poorly with test data\n",
    "# batch size of 32 is pretty common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba9c3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3 neurons with 4 inputs (this is a layer)\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5], # each batch contains outputs from 4 neurons in the previous layer\n",
    "            [2.0, 5.0, -1.0, 2.],\n",
    "            [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]\n",
    "          ]\n",
    "biases = [2.0, 3.0, 0.5]  # we added inputs becuase we are doing batches. we don't change the weights and biases;\n",
    "                            # these are essentially our layer\n",
    "    \n",
    "# second layer\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "           [-0.5, 0.12, -0.33],\n",
    "           [-0.44, 0.73, -0.13]\n",
    "          ]\n",
    "biases2 = [-1, 2.0, -0.5]\n",
    "\n",
    "# layer_outputs = []  # output of current layer\n",
    "# for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "#     neuron_output = 0  # output of given neuron\n",
    "#     for n_input, weight in zip(inputs, neuron_weights):\n",
    "#         neuron_output += n_input*weight\n",
    "#     neuron_output += neuron_bias\n",
    "#     layer_outputs.append(neuron_output)\n",
    "\n",
    "# print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46acf944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "# dot product of a layer of neurons\n",
    "weights = np.asarray(weights)\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases  # this won't work if weights and inputs are the same shape. needs to be nxm mxa so we use transpose.\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cf746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
